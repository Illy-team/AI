{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c1607e-62ef-4a8b-8780-66a9779740e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CSV 파일 경로 지정\n",
    "csv_file_path = '/Users/jhkim97/Downloads/translated-activity-output.csv'\n",
    "\n",
    "# CSV 파일 읽어오기\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 정제된 데이터를 저장할 새로운 DataFrame 생성\n",
    "cleaned_df = df.copy()\n",
    "\n",
    "# 정규 표현식을 사용하여 특수문자 제거\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 영어, 숫자, 한글, 공백을 제외한 모든 문자를 제거\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9가-힣\\s]', '', text)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        # 문자열이 아닌 경우 그대로 반환\n",
    "        return text\n",
    "\n",
    "# 모든 셀에 대해 특수문자 제거 함수를 적용\n",
    "for column in cleaned_df.columns:\n",
    "    cleaned_df[column] = cleaned_df[column].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8cbf334-70a8-48cd-ba40-23b10a51d370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jjzha/jobspanbert-base-cased were not used when initializing BertModel: ['cls.span_predictions.query_end_transform.LayerNorm.bias', 'cls.span_predictions.end_transform.LayerNorm.weight', 'cls.span_predictions.end_transform.dense.weight', 'cls.span_predictions.start_transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.span_predictions.end_transform.dense.bias', 'cls.span_predictions.end_classifier', 'cls.span_predictions.query_end_transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.span_predictions.query_start_transform.dense.bias', 'cls.span_predictions.query_start_transform.LayerNorm.bias', 'cls.span_predictions.end_transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.span_predictions.start_classifier', 'cls.predictions.decoder.weight', 'cls.span_predictions.query_end_transform.dense.weight', 'cls.span_predictions.start_transform.dense.weight', 'cls.span_predictions.start_transform.dense.bias', 'cls.span_predictions.start_transform.LayerNorm.bias', 'cls.span_predictions.query_end_transform.LayerNorm.weight', 'cls.span_predictions.query_start_transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.span_predictions.query_start_transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT Portfolio 1:\n",
      "  - Activity 275, Similarity: 0.7595\n",
      "  - Activity 294, Similarity: 0.7254\n",
      "  - Activity 298, Similarity: 0.7234\n",
      "  - Activity 274, Similarity: 0.7225\n",
      "  - Activity 276, Similarity: 0.7109\n",
      "  - Activity 285, Similarity: 0.6991\n",
      "  - Activity 288, Similarity: 0.6947\n",
      "  - Activity 284, Similarity: 0.6945\n",
      "  - Activity 289, Similarity: 0.6912\n",
      "  - Activity 290, Similarity: 0.6889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "top_10_similarities = []\n",
    "\n",
    "# activity_output.csv 파일에서 데이터 읽어오기\n",
    "# 가정: cleaned_df는 이미 'activity' 데이터를 포함하고 있음\n",
    "data = cleaned_df\n",
    "data['preferred'].fillna(\"hello\", inplace=True)\n",
    "data['requirement'].fillna(\"hello\", inplace=True)# 'preferred' 컬럼에 누락된 값이 있을 경우 기본값으로 채움\n",
    "\n",
    "# IT 포트폴리오 데이터 읽어오기\n",
    "it_portfolio_df = pd.read_csv('/Users/jhkim97/Downloads/userid_1_filtered_correct_data.csv')  # 파일 경로 수정 필요\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "model_name = \"jjzha/jobspanbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# IT 포트폴리오 데이터를 기반으로 유사도 측정\n",
    "for index, row in it_portfolio_df.iterrows():\n",
    "    role_text = row['Role']\n",
    "    project_title_text = row['ProjectTitle']\n",
    "    technology_text = row['Technology']\n",
    "\n",
    "    # IT 포트폴리오 데이터를 하나의 텍스트로 합치기\n",
    "    it_portfolio_text = f\"{role_text}. {project_title_text}. {technology_text}\"\n",
    "\n",
    "    # 포트폴리오 텍스트 토큰화 및 인덱싱\n",
    "    user_inputs = tokenizer(it_portfolio_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # activity 데이터를 처리하고 모델에 입력으로 전달\n",
    "    combined_embeddings = []\n",
    "\n",
    "    for idx, activity_row in data.iterrows():\n",
    "        requirement_text = activity_row['requirement']\n",
    "        preferred_text = activity_row['preferred']\n",
    "        category_text = str(activity_row['category'])  # category_text가 단일 객체인 경우 문자열로 변환\n",
    "\n",
    "        # 텍스트 토큰화 및 인덱싱\n",
    "        requirement_inputs = tokenizer(requirement_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        preferred_inputs = tokenizer(preferred_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        category_inputs = tokenizer(category_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # 모델에 전달하여 임베딩 얻기\n",
    "        requirement_outputs = model(**requirement_inputs)\n",
    "        preferred_outputs = model(**preferred_inputs)\n",
    "        category_outputs = model(**category_inputs)\n",
    "\n",
    "        # 임베딩을 합치기\n",
    "        combined_requirement_embedding = requirement_outputs.last_hidden_state[:, 0, :]\n",
    "        combined_preferred_embedding = preferred_outputs.last_hidden_state[:, 0, :]\n",
    "        combined_category_embedding = category_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # 가중치를 적용하여 임베딩 결합\n",
    "        weighted_combined_embedding = (0.4 * combined_requirement_embedding + 0.5 * combined_preferred_embedding + 0.3 * combined_category_embedding) / 1.2\n",
    "\n",
    "        combined_embeddings.append(weighted_combined_embedding)\n",
    "\n",
    "    # IT 포트폴리오 텍스트의 임베딩 얻기\n",
    "    it_portfolio_embedding = model(**user_inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "    # 각 요구사항, 선호사항 및 category와 IT 포트폴리오 간의 가중치 적용된 코사인 유사도 계산\n",
    "    similarities = [1 - cosine(it_portfolio_embedding.detach().numpy().flatten(), weighted_combined_embedding.detach().numpy().flatten()) for weighted_combined_embedding in combined_embeddings]\n",
    "\n",
    "    # 유사도를 기준으로 상위 10개 활동을 저장\n",
    "    top_10_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:10]\n",
    "    top_10_activities = [(activity_index, similarities[activity_index]) for activity_index in top_10_indices]\n",
    "    top_10_similarities.append((it_portfolio_text, top_10_activities))\n",
    "\n",
    "# 상위 10개\n",
    "    # 상위 10개 유사도를 출력\n",
    "    for idx, (portfolio_text, top_10_activities) in enumerate(top_10_similarities):\n",
    "        print(f\"IT Portfolio {idx + 1}:\")\n",
    "    for activity_index, similarity in top_10_activities:\n",
    "        print(f\"  - Activity {activity_index + 273}, Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1cb4c4-6e4b-4804-81ff-bc3655a45007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b2fcc-f240-4380-8a0b-df69ad033b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
