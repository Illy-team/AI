# AI

공고 추천을 위한
Transformer(BERT) based NLP modeling

유저의 포트폴리오와 채용 공고, 대외활동, 공모전등 커리어와 관련된 활동을 추천해주기 위해 자연어분석(NLP) 기법이 필요합니다

BERT(Bidirectional Encoder Representations from Transformers) 모델은 다음과 같은 결과를 갖습니다.

1. 양방향 컨텍스트 이해: BERT는 모든 단어를 주변 단어의 맥락에서 동시에 이해할 수 있는 양방향 트랜스포머 구조를 사용합니다. 이는 이전의 단방향 또는 부분적 양방향 모델과 비교하여 문장의 의미를 더 잘 파악할 수 있게 해줍니다.
2. 전이 학습의 용이성: BERT는 대규모 텍스트 코퍼스로 사전 학습된 후, 소량의 데이터로 미세 조정을 통해 다양한 NLP 작업에 적용될 수 있습니다. 이는 상대적으로 작은 데이터셋으로도 높은 성능을 달성할 수 있게 해줍니다.
3. 다양한 NLP 작업에서의 높은 성능: BERT는 자연어 이해(NLU) 작업뿐만 아니라, 질의 응답(QA), 감정 분석, 텍스트 분류, 이름이 지정된 엔티티 인식(NER) 등 다양한 작업에서 탁월한 성능을 보여줍니다.
4. 언어 모델의 새로운 기준 설정: BERT는 많은 NLP 벤치마크에서 새로운 최고 기록을 세웠으며, 이후 등장한 많은 모델들이 BERT를 기반으로 발전하였습니다.
5. 높은 이해도의 언어 표현: BERT는 단어, 문장, 그리고 그 사이의 관계를 포함한 풍부한 언어 표현을 학습합니다. 이는 복잡한 언어 이해 작업을 수행할 때 유리합니다.
6. 언어의 미묘한 차이 파악: 동음이의어와 같이 같은 단어가 다른 맥락에서 다른 의미를 가질 때, BERT는 해당 단어의 의미를 맥락에 따라 정확히 파악할 수 있습니다.

추후 유저의 지속적으로 업데이트된 포트폴리오 데이터, 추천된 공고 중에서 관심 없음으로 분류된 데이터, 관심 공고로 분류된 데이터 등의 추가 학습을 통한 파라미터 조정에 용이하다고 판단되어 사용하게되었다.

Huggingface Transformers에서 배포되는 pre-trained 모델인 jjzha/jobspanbert-base-cased을 활용하여 채용의 관점에서 보다 fit한 토큰화 및 임베딩을 하고자하였다

모델에 관한 참고 사이트
https://huggingface.co/jjzha/jobspanbert-base-cased/blob/main/README.md

공고에 포함된 지원자격, 우대사항, 카테고리에 대한 가중치를 다르게 주어 유의미한 유사도를 검출하였지만 유저의 관심/관심없음으로 라벨링된 데이터를 생성하지 못하기에 추후 지속적인 학습이 필요하다.

축적된 데이터를 바탕으로 공고간의 유사도 분석, 유저의 관심사와 공고의 유사도 분석을 통해 추천 서비스 모델의 고도화를 진행할 계획이다.
